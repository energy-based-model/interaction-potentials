
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>


<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:1.2cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}


@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> Inferring Relational Potentials in Interacting Systems</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Inferring Relational Potentials in Interacting Systems"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@du_yilun">
        <meta name="twitter:title" content="Inferring Relational Potentials in Interacting Systems">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Inferring Relational Potentials in Interacting Systems
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
               Armand Comas<sup>1</sup>
                <a href="https://yilundu.github.io/">Yilun Du<sup>2</sup></a>,
                Christian Fernandez<sup>1</sup>
                <a href="https://sandeshgh.com/">Sandesh Ghimire<sup>1</sup></a>,
                <a href="https://scholar.google.com/citations?user=hbNllP0AAAAJ&hl=en">Mario Sznaier<sup>1</sup></a>,
                <a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en">Joshua B. Tenenbaum<sup>2</sup></a>,
                <a href="https://scholar.google.com/citations?user=htt9T1AAAAAJ&hl=en">Octavia Camps<sup>1</sup></a>
<!--                <a href="https://benanne.github.io/about/">Sander Dieleman<sup>2</sup></a>,-->
<!--                <a href="https://scholar.google.com/citations?user=GgQ9GEkAAAAJ&hl=en">Rob Fergus<sup>2</sup></a>,-->
<!--                <a href="http://www.sohldickstein.com/">Jascha Sohl-Dickstein<sup>3</sup></a>,-->
<!--                <a href="https://www.stats.ox.ac.uk/~doucet/">Arnaud Doucet<sup>2</sup></a>,-->
<!--                <a href="http://www.cs.toronto.edu/~wgrathwohl/">Will Grathwohl<sup>2</sup></a>-->
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> Northeastern University</span>
            <span><sup>2</sup> MIT</span>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><b>ICML 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2302.11552">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
<!--            <a class="paper-btn" href="https://colab.research.google.com/drive/1jvlzWMc6oo-TH1fYMl6hsOYfrcQj2rEs?usp=sharing">-->
<!--                <span class="material-icons"> code </span> -->
<!--                 Colab-->
<!--            </a>-->
<!--            <a class="paper-btn" href="https://github.com/yilundu/reduce_reuse_recycle">-->
<!--                <span class="material-icons"> code </span>-->
<!--                Code-->
<!--            </a>-->
            </div>
        </div>
    </div>

    <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/trimmed_add_pot_video.mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>

    
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
            Systems consisting of interacting agents are prevalent in the world, ranging from dynamical systems in physics to complex biological networks. To build systems which can interact robustly in the real world, it is thus important to be able to infer the precise interactions governing such systems.  Existing approaches typically discover such interactions by explicitly modeling the feed-forward dynamics of the trajectories. In this work, we propose Neural Interaction Inference with Potentials (NIIP) as an alternative approach to discover such interactions that enables greater flexibility in trajectory modeling: it discovers a set of relational potentials, represented as energy functions, which when minimized reconstruct the original trajectory.
            NIIP assigns low energy to the subset of trajectories which respect the relational constraints observed.
            We illustrate that with these representations NIIP displays unique capabilities in test-time. First, it allows trajectory manipulation, such as interchanging interaction types across separately trained models, as well as trajectory forecasting. Additionally, it allows adding external hand-crafted potentials at test-time. Finally, NIIP enables the detection of out-of-distribution samples and anomalies without explicit training.
            </p>
        </div>
    </section>
    <section id="method"/>
        <hr>
        <h2>Method</h2>


        <div class="flex-row">
            <p> 
                <b>Neural Interaction Inference with Potentials</b>
                </p>
                <p>
                We present Neural Interaction Inference with Potentials (NIIP), our  unsupervised approach to decompose a trajectory  $ \mathbf{x}(1...T)_i  $, consisting of  $ N  $ separate nodes at each timestep,  into a set of separate EBM  $ E_\theta^j(\mathbf{x})  $ potentials.  NIIP is composed by two steps: <b>(i)</b> an encoder for obtaining a set of potentials and <b>(ii)</b> a sampling process which optimizes for a predicted trajectory, minimizing the inferred potentials. Energy functions in NIIP are trained using autoencoding.
                </p>
                <p>
                <b>Relational Potentials.</b>
                To effectively parameterize different potentials for separate interactions, we learn a latent conditioned energy function  $ E_\theta(\mathbf{x}, \mathbf{z}) : \mathbb{R}^{T \times D} \times \mathbb{R}^{D_z} \xrightarrow{} \mathbb{R}$. Then, inferring a set of different potentials corresponds to inferring a latent  $ \mathbf{z} \in \mathbb{R}^{D_z}  $ that conditions an energy function.
                Given a trajectory  $ \mathbf{x}(1...T)_i $, we infer a set of  $ L $ different latent vectors for each directed pair of interacting nodes in a trajectory. Thus, given a set of  $ N $ different nodes, this corresponds to a set of  $ N(N-1)L $ energy functions.
                </p>
                <p>
                To generate a trajectory, we optimize the energy function  $ E(\mathbf{x}) = \sum_{ij,l} E_\theta^{ij,l}(\mathbf{x};\mathbf{z}_{ij,l})  $, across node indices  $ i $ and  $ j $ from  $ 1 $ to  $ N $ and latent vectors  $ l  $ from  $ 1  $ to  $L  $. However, assigning one energy function to each latent code becomes prohibitively expensive as the number of nodes in a trajectory increases. Thus, to reduce this computational burden, we parameterize  $L  $ energy functions as shared message passing graph networks, grouping all edge contributions  $ ij  $ in a single network. The energy is then computed as a summation over all individual node energies after message passing.
                To evaluate the energy corresponding to a single edge factor  $ \mathbf{z}_{ij,l} $ we mask out the contributions of all other edges to the final node energies.
                To condition to message passing shared graph network on each inferred latent  $ \mathbf{z}_{ij,l} $, each edge  $ e(i,j)  $ in the graph is conditioned by the corresponding encoded edge latent code  $ \mathbf{z}_{ij,l}  $, by means of FiLM modulation.
                </p>
                <p>
                <b>Inferring Energy Potentials.</b>
                We utilize  $ \text{Enc}_{\theta}(\mathbf{x}): \mathbb{R}^{T \times D} \xrightarrow{} \mathbb{R}^{D_z}   $ to encode the observed trajectories  $ \mathbf{x}  $ into  $ L  $ latent representations per edge in the observation. We utilize a fully connected GNN with message-passing to infer latents. Instead of classifying edge types and using them as a gate ouputs,  we utilize a continuous latent code  $ \mathbf{z}_{ij,l}  $, allowing for higher flexibility.
                </p>
                <p>
                    <b>Training Objective.</b>
                    To train NIIP, we infer a set of different EBM potentials by auto-encoding the underlying trajectory. In particular, given a  trajectory  $ \mathbf{x}(1...T)_i=\left(\mathbf{x}(1)_i,\dots,\mathbf{x}(T)_i\right)  $, we split the trajectory into initial conditions   $ \mathbf{x}(1...T_0)  $, corresponding to the first  $ T_0  $ states of the trajectory and  $ \mathbf{x}(T_0...T)  $, corresponding to the subsequent states of the trajectory, where each state of the trajectory consists of  $ N  $ different nodes. The edge potentials are encoded by observing a portion of the overall trajectory  $ \mathbf{x}(1...T^\prime)  $, where  $ T^\prime \leq T  $.
                    We infer a set of different  $ L  $ latents per edge of input observations utilizing the observed states  $ \mathbf{x}(1...T^\prime)  $ using the encoder specified in \sect{sect:inference}, generating a set of latents  $ \{\mathbf{z}\}  $. We then aim to train energy functions so that the following unnormalized distribution assigns low energy and high likelihood to the full trajectory  $ \mathbf{x}  $:
                </p>
                <p>
                \begin{equation}
                    p(\mathbf{x}|\{\mathbf{z}\}) \propto \prod_{i,j,l \forall i \neq j} {p(\mathbf{x}|\mathbf{z}_{ij,l})} =
                    \nonumber = \text{exp}\left({-E_\theta^{ij,l}(\mathbf{x};\text{Enc}_{\theta}(\mathbf{x}(1...T^\prime))_{ij,l}}) \right),
                \end{equation}
                where  $ \mathbf{z}_{ij,l} = \text{Enc}_{\theta}(\mathbf{x}(1...T^\prime))_{ij,l}  $ and  $ E_\theta^{ij,l}  $ is the energy function linked to the  $ l_\text{th}  $ potential of the encoded edge between nodes  $ i  $ and  $ j  $, respectively.
                </p>
                <p>
                Since we wish to learn a set of potentials with high likelihood for the observed trajectory  $ \mathbf{x}  $, as a tractable supervised manner to learn such a set of valid potentials, we directly supervise that sample using \eqn{eq:mixing} corresponds to the original trajectory  $ \mathbf{x}  $. In particular, we sample  $ M  $ steps of Langevin sampling starting from  $ \tilde{\mathbf{x}}^0  $, which is initialized from uniform noise and the initial conditions fixed as the ground-truth  $ \mathbf{x}(1...T_0)  $:
                </p>
                <p>
                \begin{equation}
                 \tilde{\mathbf{x}}^m =   \tilde{\mathbf{x}}^{m-1}- \frac{\lambda}{2}\nabla_{\mathbf{x}}\sum_{ij,l}{E_\theta^{ij,l}(\tilde{\mathbf{x}}^{m-1};\mathbf{z}_{ij,l}} ) + \omega^m, \label{eq:langevin-update}
                \end{equation}
                </p>
                <p>
                where  $ m  $ is the  $ m_\text{th}  $ step and  $ \lambda  $ is the step size and  $ \omega^m \sim \mathcal{N}(0, \lambda)  $. We then compute MSE objective with  $ \tilde{\mathbf{x}}^M  $, which is the result of  $ M  $ sampling iterations and the ground truth trajectory  $ \mathbf{x}  $:
                </p>
                <p>
                \begin{equation}
                  \mathcal{L}_{\text{MSE}}(\theta) = \| \tilde{\mathbf{x}}^M- \mathbf{x} \|^2.
                \end{equation}
                We optimize both  $ \tilde{\mathbf{x}}  $ and the parameters  $ \theta  $ with automatic differentiation..
                </p>
        </div>
    </section>




        

    <section id="results">
        <hr>
        <h2>Recombination</h2>

        <p>
        <div class="flex-row">
                <p>NIIP can compose relational potentials learned from two different distributions, at test-time.
                    The process is as follows: we train two instances of our model ( $ \text{NIIP}_S  $,  $ \text{NIIP}_C  $) to reconstruct Springs and Charged trajectories respectively. Given sample trajectories drawn from each dataset (Col. 1 for Springs and 2 for Charged), we encode them into their relational potentials. For each row, we aim to reconstruct the trajectory framed in green while swapping one of the potential pairs (green dashed box) with one drawn from the other dataset (red dashed box). As an example, in the first row of the figure, we encode the Springs trajectory with  $ \text{NIIP}_S  $ and the Charged trajectory with  $ \text{NIIP}_C  $. Next, we fix the initial conditions of the Charged trajectories and sample by optimizing the relational energy functions. To achieve recombination, each model targets specific edges. We minimize the potentials encoded by  $ \text{NIIP}_S  $ for the mutual edges corresponding to the nodes in green dashed boxes. The rest of edge potentials are encoded by  $ \text{NIIP}_C  $. The sampling process is done jointly by both models, each minimizing their corresponding potentials.
                    The result is a natural combination of the two datasets, which affect only the targeted edges.
				</p>
            </div> 
            <center>
            <figure>
                <a>
                    <img width="70%" src="materials/intermix.png">
                </a>
                <p class="caption">
                    <br>NIIP can recombine encoded potentials at test-time learned from different datasets. </br> Illustrated, samples from Springs (Col. 1) and Charged (Col. 2) and their recombinations (Col. 3). NIIP encodes both trajectories. NIIP is able to reconstruct trajectories framed in green while swapping the edge potentials associated to the nodes in the green dashed box for the ones in the red dashed boxes. Recombinations look semantically plausible.
                </p> <br>
            </figure>
            </center>

        <hr>


        <h2>Out-Of-Distribution Detection</h2>
            <div class="flex-row">
                <p>We further utilize the potential value or energy produced by NIIP over a trajectory to detect out-of-distribution interactions in a trajectory. In our proposed architecture, energy is evaluated at the node level. Therefore, if NIIP has been trained with a specific dataset, the potentials associated to out-of-distribution type of edges are expected to correspond to higher energy.
We design a new dataset (Charged-Springs) as a combination of Springs and Charged interaction types. In simulation, nodes are assigned both roles of Charged and Springs particles, but all the forces they receive correspond to one of the two types with probability  $ p=0.5  $. We train a model with the Springs dataset and evaluate the energies in the proposed mixed setting.
Figure \ref{fig:ood} shows qualitatively how the energy is considerably higher for the nodes with Charged-type forces (drawn in red). Quantitative results are summarized in Table \ref{tab:ood} for 1k test samples. In the left, we can see that energies corresponding to Spring-type nodes are considerably lower than for Charged-type nodes, indicating that potentials are correctly capturing the behavior of the desired interactions.
We further evaluate the OOD detection for NBA SportsVU dataset. For this experiment, we train NIIP with the 10 players disregarding the Ball node. At test-time, we evaluate the trained model switching one of the players by the Ball node for 1k samples. We observe how the energy corresponding to the Ball is considerably higher. We further train a single parameter binary classifier and find that we can detect the Ball in 70.1\% of instances (Table 1 right).
</p>
            </div> 

        <center>
            <figure>
                <a>
                    <img width="60%" src="materials/ood.png">
                </a>
                <p class="caption">
                    <b>Out-Of-Distribution Detection with NIIP</b>. A model trained with certain relation types can detect when a trajectory exhibits a new type of relation. The illustrated trajectories show the energy associated to each one of the nodes. Red trajectories: Charged particles, Blue-Green trajectories: Springs particles. We train NIIP in Springs dataset. Our model assigns higher energies to those nodes that behave differently than the training set.
                </p> <br>
            </figure>
            </center>
            <center>
            <figure>
                <a>
                    <img width="60%" src="materials/ood-table.png">
                </a>
                <p class="caption">
                    <b>Quantitative evaluation of out-of-distribution detection</b>.  We evaluate the average energy associated to each node-type in a scene. In the left, NIIP is trained on Springs and evaluated onn (i) Springs (ii) Charged and (iii) S\&C, a Springs-Charged mixed dataset. For the NBA case in the right, we train NIIP for the subset of player (P) trajectories of the dataset and evaluate the energies in a setting with player nodes and one ball node. We measure accuracy in detecting the ball trajectory.
                </p> <br>
            </figure>
            </center>


        <hr>

        <h2>Flexible Generation</h2>
            <div class="flex-row">
                <p>Another advantage of our approach is that it can flexibly incorporate test-time user specified potentials. For this experiment, we investigate three different sets of potentials.
                    <p>
                    <b>Velocity Potentials</b> We incorporate the following velocity potential as an energy function:  $ E = \epsilon \lambda \sum_{i,t}\sqrt{(\mathbf{v}_{x,i}^t)^2 + (\mathbf{v}_{y,i}^t)^2} = \epsilon \lambda\sum_{i,t}mod(\mathbf{v}_{i}^t)  $, for particle  $ i  $ in time  $ t  $. The weight  $ \lambda =1e-2/{N}  $ scales the effect of this function over the rest and  $ \epsilon  $ is a multiplicative constant that indicates the strength and direction of the potential. Figure \ref{fig:new-constraints} (two top rows), we show (\textbf{i.})  $ \epsilon = 0  $: Reconstruction (top-left); (\textbf{ii.})  $ \epsilon = 4  $: Decrease of velocity (middle-left); (\textbf{iii.})  $ \epsilon = -5  $: Low increase of velocity (top-right); and (\textbf{iv.})  $ \epsilon = -10  $: High increase of velocity (middle-right)
                </p>
                <p>
                    <b>Goal Potentials</b> We also add at test-time an attraction potential as a the squared distance of the predicted coordinates to the goal:  $ P = \epsilon\lambda \sum_{i,t}(\tilde{\mathbf{p}}^{0:T-1}_i - \mathbf{g})^2  $, where  $ \mathbf{g}  $ is defined as the coordinates of our goal point. We define the trajectory coordinates  $ \tilde{\mathbf{p}}^{t}_i  $ as an accumulation of the un-normalized velocities  $ \mathbf{v}^{t}_i  $ predicted by NIIP:  $ \tilde{\mathbf{p}}^{t+1}_i = \sum_{t}(\mathbf{v}^{0:t}_i) + \mathbf{p}^0_i  $ for particle  $ i  $ at time-step  $ t  $.  Here,  $ \mathbf{p}^1  $ is fixed initial ground-truth location of the particle at time 0 and  $ \lambda =5e-4/{N}  $.
                </p>
                <p>
                We explore quantitatively the effect of different magnitudes of the added goal potential for prediction in the Charged dataset. Our test set is composed by 1k samples.
                For this experiment, the particles live within a  $ [-1,1]  $ box, for both  $ x  $ and  $ y  $ coordinates. The goal is the center  $ \mathbf{g} = (0,0)  $. In the table,  $ P_{add}  $ indicates the use of edge potentials encoded by NIIP while adding the new potentials with different strengths: (\textbf{i.})  $ s1: \epsilon=1  $, (\textbf{ii.})  $ s2: \epsilon=5  $ and (\textbf{iii.})  $ s3: \epsilon=10  $. We observe how the squared distance to the goal decreases as expected.
                </p>
                <p>
                    <b>Avoid Area Potentials</b> In this case, we penalize the portion of the predicted trajectory  $ \tilde{\mathbf{p}}^{t}_i  $ that is inside a given restricted area  $ \mathbf{A}  $. We do so by computing the distance of each particle  $ i  $ that lays within the region  $ \mathbf{A}  $ to the borders  $ \mathbf{b}_A  $ of  $ \mathbf{A}  $. The added potential is:  $ P = \epsilon\lambda \sum_{i,t}(\tilde{\mathbf{p}}_{A,i}^{0:T-1} - \mathbf{b}_A - C)^2  $, where  $ C  $ is a small margin that ensures that the particles are repelled outside of the boundaries of  $ \mathbf{A}  $. With  $ \lambda =1e-3/{N}  $.
                In the Table below (row 2), we explore the effect of different strengths of this potential type, in the prediction task. We also provide a visual example. For this experiment, we avoid the area  $ \mathbf{A} = [(0, -1), (1, 1)]  $, which corresponds to half of the box (see Figure). We use the following parameters: (\textbf{i.})  $ s1: \epsilon=1  $, (\textbf{ii.})  $ s2: \epsilon=5e1  $ and (\textbf{iii.})  $ s3: \epsilon=5e2 $ .
                </p>

                <center>
                    <figure>
                        <a>
                            <img width="60%" src="materials/new-potentials-charged.png">
                        </a>
                        <p class="caption">
                            <b>With NIIP we can add and control test-time potentials to achieve a desired behavior</b>. We design test-time potentials to steer trajectories into a goal. In row 1 of the table, we show the squared distance after applying a goal potential towards the center (0,0) of the scene with different strengths. In row 2, we report the percentage of time-steps that particles stay in a particular area A = [(0, −1), (1, 1)] after applying a potential that enforces avoiding A. The figure shows the effect of applying test-time potentials in the latter experiment.
                        </p> <br>
                    </figure>
                </center>
                <center>
                    <figure>
                        <a>
                            <img width="60%" src="materials/new-potentials-table.png">
                        </a>
                        <p class="caption">
                            <b>Quantitative evaluation of out-of-distribution detection</b>. We evaluate the average energy associated to each node-type in a scene. In the left, NIIP is trained on Springs and evaluated onn (i) Springs (ii) Charged and (iii) S&C, a Springs-Charged mixed dataset. For the NBA case in the right, we train NIIP for the subset of player (P) trajectories of the dataset and evaluate the energies in a setting with player nodes and one ball node. We measure accuracy in detecting the ball trajectory.
                        </p> <br>
                    </figure>
                </center>
                <center>
                    <figure>
                        <a>
                            <img width="100%" src="materials/new-potentials-nba.png">
                        </a>
                        <p class="caption">
                            <b>NIIP is able to incorporate new potentials in test-time</b>. We can see depicted reconstructions of NBA samples with added potentials.
                        </p> <br>
                    </figure>
                </center>
            </div>
        <hr>


        <h2>Forecasting</h2>
            <div class="flex-row">
                <p>We asses NIIP's capabilities for trajectory forecasting.  NIIP can predict trajectories faithfully in the future, displaying favorable mid- and long-term performance when compared to existing approaches.
                </p>
            </div> 

            <center>
            <figure>
                <a>
                    <img width="49%" src="materials/Prediction_ Springs Dataset.png">
                </a>
                <p class="caption">
                </p> <br>
            </figure>
            </center>
        <center>
            <figure>
                <a>
                    <img width="49%" src="materials/Prediction_ Charged Dataset.png">
                </a>
                <p class="caption">
                </p> <br>
            </figure>
                        <figure>
                <a>
                    <img width="49%" src="materials/Prediction_ NBA SportsVU Dataset.png">
                </a>
                <p class="caption">
                </p> <br>
            </figure>
                        <figure>
                <a>
                    <img width="49%" src="materials/Prediction_ JPL Horizons Dataset.png">
                </a>
                <p class="caption">
                </p> <br>
            </figure>
            </center>


    </section> 




<!--    <section id="related_projects">-->
<!--        <hr>-->
<!--        <h2>Related Projects</h2>  -->

<!--          <br>-->
<!--          Check out a list of our related papers on compositional generation and energy based models. A full list can be found <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">here</a>!-->
<!--          <br>-->

<!--        <div class="row vspace-top">-->
<!--        <div class="col-sm-3">-->
<!--            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">-->
<!--                <source src="materials/related/teaser_glide.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--        <div class="col-sm-9">-->
<!--          <div class="paper-title">-->
<!--            <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Compositional Visual Generation with Composable Diffusion Models</a>-->
<!--        </div>-->
<!--        <div>-->
<!--            We present a method to compose different diffusion models together, drawing on the close connection of-->
<!--            diffusion models with EBMs. We illustrate how compositional operators enable-->
<!--            the ability to composing multiple sets of objects together as well as generate images subject to -->
<!--            complex text prompts.-->
<!--        </div>-->
<!--        </div>-->
<!--        </div>-->

<!--        <div class="row vspace-top">-->
<!--        <div class="col-sm-3">-->
<!--            <div class="move-down">-->
<!--                <img src="materials/related/comp_cartoon.png" class="img-fluid" alt="comp_carton" style="width:100%">-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="col-sm-9">-->
<!--          <div class="paper-title">-->
<!--            <a href="https://energy-based-model.github.io/compositional-generation-inference/">Compositional Visual Generation with Energy Based Models</a>-->
<!--        </div>-->
<!--        <div>-->
<!--            We present a set of compositional operators that enable EBMs to exhibit <b>zero-shot compositional</b> visual generation, enabling us to compose visual concepts-->
<!--            (through operators of conjunction, disjunction, or negation) together in a zero-shot manner.-->
<!--            Our approach enables us to generate faces given a  description-->
<!--            ((Smiling AND Female) OR (NOT Smiling AND Male)) or to combine several different objects together.-->
<!--        </div>-->
<!--        </div>-->
<!--        </div>-->


<!--        <div class="row vspace-top">-->
<!--        <div class="col-sm-3">-->
<!--            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">-->
<!--                <source src="materials/related/half.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--        <div class="col-sm-9">-->
<!--          <div class="paper-title">-->
<!--                        <a href="https://openai.com/blog/energy-based-models/">Implicit Generation and Generalization with Energy Based Models</a>-->
<!--        </div>-->
<!--                We introduce a method to scale EBM training to generate high resolution images.-->
<!--                We propose to utilize Langevin dynamics, initialized from random noise, to iteratively-->
<!--                refine and denoise image samples. We further demonstrate unique properties of EBMs-->
<!--                such as compositionality, continual learning, and robustness.-->
<!--        <div>-->
<!--        </div>-->
<!--        </div>-->

<!--    </section> -->


    <section id="paper">
        <h2>Team</h2>        
        <div class="row">
            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/comas-a.jpeg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Armand Comas </p>
                <p class=institution>Northeastern University</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/yilun3.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Yilun Du </p>
                <p class=institution>MIT CSAIL</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/fernandez-c.jpeg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Christian Fernandez </p>
                <p class=institution>Northeastern University</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/ghimire-s.jpeg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Sandesh Ghimire </p>
                <p class=institution>Northeastern University</p>
            </div>
         </div>
         <div class="row">
            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/sznaier-m.jpeg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Mario Sznaier </p>
                <p class=institution>Northeastern University</p>
            </div>

            <div class="column5">
                <a href='https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en'>
                    <img  src=./materials/people/Tenenbaum-250x250px.jpeg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Joshua Tenenbaum </p>
                <p class=institution>MIT CSAIL</p>
            </div>

            <div class="column5">
                <a href="">
                    <img src=./materials/people/camps-o.jpeg class="figure-img img-fluid rounded-circle" height=100px width=200px>
                </a>
                <p class=profname> Octavia Camps </p>
                <p class=institution>Northeastern University</p>
            </div>
         </div>

    </section>
   
    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
        <center><p><a href='https://accessibility.mit.edu/'><b>Accessibility</b></a></p></center>
    </section>
    


</div>
</body>
</html>
